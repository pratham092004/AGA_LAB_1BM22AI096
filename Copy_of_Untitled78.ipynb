{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQwOTE6YP5jt",
        "outputId": "07164934-2083-475f-9969-8eec1ef398a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.4811\n",
            "Epoch 2, Loss: 0.4811\n",
            "Epoch 3, Loss: 0.4810\n",
            "Epoch 4, Loss: 0.4810\n",
            "Epoch 5, Loss: 0.4811\n",
            "Epoch 6, Loss: 0.4811\n",
            "Epoch 7, Loss: 0.4809\n",
            "Epoch 8, Loss: 0.4810\n",
            "Epoch 9, Loss: 0.4810\n",
            "Epoch 10, Loss: 0.4810\n",
            "(60000, 128) (60000,)\n",
            "(10000, 128) (10000,)\n",
            "Classification Accuracy: 0.1339\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "train_data = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_data = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, visible_units, hidden_units):\n",
        "        super(RBM, self).__init__()\n",
        "        self.W = nn.Parameter(torch.randn(hidden_units, visible_units) * 0.01)\n",
        "        self.h_bias = nn.Parameter(torch.zeros(hidden_units))\n",
        "        self.v_bias = nn.Parameter(torch.zeros(visible_units))\n",
        "\n",
        "    def sample_hidden(self, v):\n",
        "        prob_h = torch.sigmoid(torch.matmul(v, self.W.t()) + self.h_bias)\n",
        "        return torch.bernoulli(prob_h), prob_h\n",
        "\n",
        "    def sample_visible(self, h):\n",
        "        prob_v = torch.sigmoid(torch.matmul(h, self.W) + self.v_bias)\n",
        "        return torch.bernoulli(prob_v), prob_v\n",
        "\n",
        "    def contrastive_divergence(self, v, k=1):\n",
        "        v_sample = v\n",
        "        for _ in range(k):\n",
        "            h_sample, _ = self.sample_hidden(v_sample)\n",
        "            v_sample, _ = self.sample_visible(h_sample)\n",
        "        return v_sample\n",
        "\n",
        "    def forward(self, v):\n",
        "        h_sample, _ = self.sample_hidden(v)\n",
        "        return h_sample\n",
        "\n",
        "rbm = RBM(visible_units=784, hidden_units=128)\n",
        "optimizer = optim.SGD(rbm.parameters(), lr=0.1)\n",
        "\n",
        "def train_rbm(epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        loss_epoch = 0\n",
        "        for data, _ in train_loader:\n",
        "            data = data.view(-1, 784)\n",
        "            v_sample = rbm.contrastive_divergence(data)\n",
        "            loss = torch.mean((data - v_sample) ** 2)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_epoch += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss_epoch/len(train_loader):.4f}\")\n",
        "\n",
        "train_rbm()\n",
        "\n",
        "def extract_features(data_loader):\n",
        "    features, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data = data.view(-1, 784)\n",
        "            h_sample, _ = rbm.sample_hidden(data)\n",
        "            features.append(h_sample.numpy())\n",
        "            labels.append(target.numpy())\n",
        "    return np.vstack(features), np.hstack(labels)\n",
        "\n",
        "X_train, y_train = extract_features(train_loader)\n",
        "X_test, y_test = extract_features(test_loader)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Classification Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, visible_units, hidden_units):\n",
        "        super(RBM, self).__init__()\n",
        "        self.W = nn.Parameter(torch.randn(hidden_units, visible_units) * 0.01)\n",
        "        self.h_bias = nn.Parameter(torch.zeros(hidden_units))\n",
        "        self.v_bias = nn.Parameter(torch.zeros(visible_units))\n",
        "\n",
        "    def sample_hidden(self, v):\n",
        "        prob_h = torch.sigmoid(torch.matmul(v, self.W.t()) + self.h_bias)\n",
        "        return torch.bernoulli(prob_h), prob_h\n",
        "\n",
        "    def sample_visible(self, h):\n",
        "        prob_v = torch.sigmoid(torch.matmul(h, self.W) + self.v_bias)\n",
        "        return torch.bernoulli(prob_v), prob_v\n",
        "\n",
        "    def contrastive_divergence(self, v, k=1):\n",
        "        v_sample = v\n",
        "        for _ in range(k):\n",
        "            h_sample, _ = self.sample_hidden(v_sample)\n",
        "            v_sample, _ = self.sample_visible(h_sample)\n",
        "        return v_sample\n",
        "\n",
        "class StackRBM:\n",
        "    def __init__(self, size=5):\n",
        "        self.stack_size = size\n",
        "        self.stack = torch.zeros((1, size))\n",
        "        self.rbm = RBM(visible_units=size, hidden_units=size)\n",
        "        self.optimizer = optim.SGD(self.rbm.parameters(), lr=0.1)\n",
        "\n",
        "    def push(self, value):\n",
        "        self.stack = torch.roll(self.stack, shifts=1, dims=1)\n",
        "        self.stack[0, 0] = value\n",
        "        self.train_rbm()\n",
        "\n",
        "    def pop(self):\n",
        "        top_value = self.stack[0, 0].item()\n",
        "        self.stack[0, 0] = 0\n",
        "        self.stack = torch.roll(self.stack, shifts=-1, dims=1)\n",
        "        self.train_rbm()\n",
        "        return top_value\n",
        "\n",
        "    def train_rbm(self):\n",
        "        v_sample = self.rbm.contrastive_divergence(self.stack)\n",
        "        loss = torch.mean((self.stack - v_sample) ** 2)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def display_stack(self):\n",
        "        print(\"Stack State:\", self.stack.numpy())\n",
        "\n",
        "stack_rbm = StackRBM(size=5)\n",
        "stack_rbm.push(1)\n",
        "stack_rbm.push(2)\n",
        "stack_rbm.push(3)\n",
        "stack_rbm.display_stack()\n",
        "print(\"Popped:\", stack_rbm.pop())\n",
        "stack_rbm.display_stack()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EZzcATgWIvN",
        "outputId": "c8d40458-3392-4e24-a061-6497f5a8072a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stack State: [[3. 2. 1. 0. 0.]]\n",
            "Popped: 3.0\n",
            "Stack State: [[2. 1. 0. 0. 0.]]\n"
          ]
        }
      ]
    }
  ]
}